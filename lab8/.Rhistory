ls
cd
getwd
getwd()
install.packages("network")
install.packages(c("BH", "bindr", "bindrcpp", "broom", "car", "caret", "cluster", "ddalpha", "glmnet", "knitr", "lava", "lfe", "lme4", "lubridate", "Matrix", "pillar", "plogr", "psych", "quantreg", "Rcpp", "RcppEigen", "rlang", "sfsmisc", "stringi", "stringr", "tibble", "tidyr", "tidyselect", "timeDate", "viridis", "viridisLite", "visreg", "withr", "yaml"), lib="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
library(quanteda)
library(tidyverse)
library(RColorBrewer)
library(stm)
install.packages("pillar")
install.packages("pillar")
install.packages("rlang")
# clear all
install.packages("quanteda")
# clear all
install.packages("quanteda")
library(tidyverse)
# clear all
install.packages("rtools")
install.packages("quanteda")
install.packages("stm")
installed.packages("tidyverse")
installed.packages("RColorBrewer")
install.packages('devtools')
devtools::install_github('IRkernel/IRkernel')
IRkernel::installspec()
# clear all
install.packages("rtools")
install.packages("quanteda", dependencies=TRUE)
remove.packages(IRkernel)
remove.packages('IRkernel')
install.packages("quanteda", dependencies=TRUE)
install.packages("XML")
install.packages("stm", dependencies=TRUE)
install.packages("sotu")
library(sotu)
#data structure
sotu <- data.frame(cbind(sotu_text, sotu_meta), stringsAsFactors=FALSE)
sotu$sotu_text <- as.character(sotu$sotu_text)
View(sotu)
sotu_text_data <- PrepText(sotu, textvar="sotu_text", groupvar="president", node_type="groups", remove_stop_words=TRUE, stem=TRUE)
#packages needed
install.packages("devtools")
library(devtools)
install_github("cbail/textnets")
library(textnets)
library(devtools)
library(devtools)
#packages needed
install.packages("devtools")
install.packages('git2r')
#install.packages('devtools') #assuming it is not already installed
library(devtools)
install_github('andreacirilloac/updateR')
library(updateR)
updateR(admin_password = 'Harv2020')
rm(list = ls())
library(quanteda)
library(tidyverse)
library(RColorBrewer)
library(stm)
setwd("~/Dropbox/coursework/2018Spring/soc220_spring2018/Github_labs/lab8")
#######################################################################################
# READ FROM EXCEL MY DATA
library(readxl)
efr_tm <- read_excel("efr_tm.xlsx")
# create a corpus object
corpus_object <- corpus(efr_tm)
# lastly, trim that matrix to remove uncommon or too common words
trim_doc_freq_matrix <- dfm_trim(doc_freq_matrix, max_docfreq=.90, min_docfreq = .2)
doc_freq_matrix <- dfm(corpus_object,
#stop words
remove = c(stopwords("english")),
#specify length of n-grams
ngrams=1L,
#stemmer? (porter)
stem = F,
#remove misc
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE)
# lastly, trim that matrix to remove uncommon or too common words
trim_doc_freq_matrix <- dfm_trim(doc_freq_matrix, max_docfreq=.90, min_docfreq = .2)
#structural topic model object from trimmed dtm
texts_stm <- convert(trim_doc_freq_matrix, to = 'stm')
texts_stm$documents
texts_stm$vocab
texts_stm$meta
#plot removed of how many words by minimal threshold count
plotRemoved(texts_stm$documents,lower.thresh = seq(1, 50, by = 1))
#``````````````
#` prep documents object which takes three arguments:
#`1. documents
#`2. the total vocabulary
#`3. meta data
#`4. lastly has another threshold if you wish to remove based on plot removed
#````
prep_docs_object <- prepDocuments(texts_stm$documents, texts_stm$vocab,
texts_stm$meta, lower.thresh = 10)
#`search for optimal number of topics.
#`Takes in documents, vocab from structural topic object
#plots log-likelihood
ntopics <- searchK(texts_stm$documents,texts_stm$vocab,
K = c(3,5,7), data = texts_stm$meta)
plot(ntopics)
#`search for optimal number of topics.
#`Takes in documents, vocab from structural topic object
#plots log-likelihood
ntopics <- searchK(texts_stm$documents,texts_stm$vocab,
K = c(3,7,12), data = texts_stm$meta)
plot(ntopics)
#`search for optimal number of topics.
#`Takes in documents, vocab from structural topic object
#plots log-likelihood
ntopics <- searchK(texts_stm$documents,texts_stm$vocab,
K = c(3,7,12,20), data = texts_stm$meta)
plot(ntopics)
#fit stm, FINALLY!
fit_stm <- stm(documents = texts_stm$documents,
vocab = texts_stm$vocab,
K = 10,
#covariates
prevalence = ~ monarch + war,
seed = 02138,
data = texts_stm$meta
)
summary(fit_stm)
#plot frequent words for given topics
plot.STM(fit_stm, type = "labels")
#plot frequent words for given topics
plot.STM(fit_stm, type = "labels")
#plot topic proportions over whole corpus
plot.STM(fit_stm, type = "summary")
par(mfrow = c(1,2))
plot(fit_stm, type = 'labels', topics = c(2, 4:5), labeltype = 'frex', main = 'FREX')
plot(fit_stm, type = 'labels', topics = c(2, 2:3), labeltype = 'score', main = 'score')
par(mfrow = c(1,1))
plot(fit_stm, type = 'perspectives',topics = c(1,2))
plot(fit_stm, type = 'perspectives',topics = c(2,3))
plot(fit_stm, type = 'perspectives',topics = c(3,4))
topic_corr = topicCorr(fit_stm)
plot(topic_corr)
#for a given outcome, we can plot differences in select topic proportions
#which topics are predicted by war
est_stm <- estimateEffect( ~ war, fit_stm, metadata = texts_stm$meta)
summary(est_stm)
plot(est_stm, covariate = 'war', topics = 5:6, model = fit_stm)
plot(est_stm, covariate = 'war', topics = c(4,9), model = fit_stm)
plot(est_stm, covariate = 'war', topics = c(1,9), model = fit_stm)
plot(est_stm, covariate = 'war', topics = 1:9, model = fit_stm)
